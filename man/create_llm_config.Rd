% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_providers.R
\name{create_llm_config}
\alias{create_llm_config}
\title{Create LLM Configuration}
\usage{
create_llm_config(
  provider,
  model,
  api_key = NULL,
  base_url = NULL,
  api_version = NULL,
  temperature = 0.1,
  max_tokens = 2000,
  timeout_seconds = 120,
  env_file = NULL
)
}
\arguments{
\item{provider}{Character. LLM provider type (see LLM_PROVIDERS)}

\item{model}{Character. Model name/identifier}

\item{api_key}{Character. API key (if required by provider). If NULL, will try to load from environment or .env file}

\item{base_url}{Character. Base URL for API (used by Ollama, Azure, and Gemini)}

\item{api_version}{Character. API version (used by Azure OpenAI)}

\item{temperature}{Numeric. Sampling temperature (0-1, default: 0.1)}

\item{max_tokens}{Integer. Maximum tokens in response (default: 2000)}

\item{timeout_seconds}{Integer. Request timeout in seconds (default: 120)}

\item{env_file}{Character. Path to .env file to load API keys from (optional)}
}
\value{
LLM configuration object
}
\description{
Creates a configuration object for connecting to various LLM providers
with appropriate authentication and model settings.
}
\examples{
\dontrun{
# OpenAI configuration
openai_config <- create_llm_config(
  provider = "openai",
  model = "gpt-4",
  api_key = Sys.getenv("OPENAI_API_KEY")
)

# Ollama configuration (local)
ollama_config <- create_llm_config(
  provider = "ollama",
  model = "llama2",
  base_url = "http://localhost:11434"
)

# Anthropic configuration
anthropic_config <- create_llm_config(
  provider = "anthropic",
  model = "claude-3-sonnet-20240229",
  api_key = Sys.getenv("ANTHROPIC_API_KEY")
)

# Gemini configuration with automatic .env loading
gemini_config <- create_llm_config(
  provider = "gemini",
  model = "gemini-1.5-flash"
)

# Gemini configuration with specific .env file
gemini_config <- create_llm_config(
  provider = "gemini",
  model = "gemini-1.5-flash",
  env_file = "path/to/.env"
)
}
}
